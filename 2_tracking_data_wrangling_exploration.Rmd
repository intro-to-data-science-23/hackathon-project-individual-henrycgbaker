---
title: "2_tracking_data_wrangling_exploration"
author: "Henry Baker"
date: "2023-12-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(dplyr)
library(haven)
library(plotly)
library(patchwork)
library(tidyr)
library(fuzzyjoin)
library(stringr)

```

```{r set up wd etc}

setwd("/Users/henrybaker/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/IDS'23/hackathon-project-individual-henrycgbaker")
options(scipen = 999)

```

## Loading data

```{r load data}

survey_data <- readRDS("survey/meof_usa_survey_df.RDS")

tracking_data <- readRDS("tracking/meof_usa_web_df.RDS")

time_zones <- readRDS("tracking/time_zones.RDS")

```

## Align with waves from survey df

This would be useful if we're doing any form of time series analysis or want to know how internet visits changes over time....

... but it crashes my comp (it works on smaller subsets so the code works, but my comp just isn't powerful enough to run it on the whole dataset)

```{r}
# create 'wave period's based on first and last observations in survey data
wave_periods <- survey_data |> 
  group_by(wave) |>
  summarise(beginning_period = first(starttime), 
            ending_period = last(endtime)
  )

# non-equi join to assign waves based on used_at timestamp
# NB: this does work when i tried it on a subset of tracking_data, it just takes a long time 
# NEED TO LET IT RUN AND THEN SAVE OUTPUT:
#tracking_data <- fuzzyjoin::fuzzy_left_join(tracking_data, wave_periods,
                                            #by = c("used_at" = "beginning_period", "used_at" = "ending_period"),
                                            #match_fun = list(`>=`, `<=`)) 
#tracking_data <- tracking_data |>
  #select(-c(beginning_period, ending_period))

```

## Initial exploration
```{r high level}

# count visits for each website
website_visits <- tracking_data |>
  group_by(domain) |>
  summarise(visit_count = n(), .groups = 'drop') |>
  arrange(desc(visit_count))

#  top 20 websites
top_websites <- head(website_visits, 20)

# Create a histogram
top_web_plot_1 <- ggplot(top_websites, aes(x = reorder(domain, visit_count), y = visit_count)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flips the axes for better readability
  labs(title = "Top 20 Most Visited Websites",
       x = "Website",
       y = "Number of Visits") +
  theme_minimal()
top_web_plot_1

# List of survey-related, ad-related, other non-insightful domains to exclude
survey_related_domains <- c("adservice.google.com", "nav.smartscreen.microsoft.com", "vid-io.springserve.net", "syn.entertainow.com", "surveyjunkie.com", "samplicio.us", "acds.prod.vidible.tv", "time.rmprod.zone", "pf.entertainow.com", "track1.aniview.com", "mypoints.com", "collect.sbkcenter.com", "s.cpx.to", "t.nav.smartscreen.microsoft.com", "t.myvisualiq.net", "otf.msn.com", "sync.graph.bluecava.com", "yougovus.wakoopa.com", "yougovus2.wakoopa.com", "thrtle.com", "fc.yahoo.com", "hlsrv.vidible.tv", "tag.cogocast.net", "inboxdollars.com")  # check these 

# Filter out non-releant-related domains
filtered_website_visits <- website_visits %>%
  filter(!domain %in% survey_related_domains)

# Select the top 20 websites after excluding  domains
top_websites <- head(filtered_website_visits, 20)

# Create a histogram
top_web_plot_2 <- ggplot(top_websites, aes(x = reorder(domain, visit_count), y = visit_count)) +
  geom_bar(stat = "identity") +
  coord_flip() +  
  labs(title = "Top 20 Most Visited Websites (Excluding Surveys, Trackers, Point Schemes, Ad-ware, and Other Non-relevant domains)",
       x = "Website",
       y = "Number of Visits") +
  theme_minimal()
top_web_plot_2

```

## Most Time Spent

```{r}

# Calculate total time spent on each website in hours
website_time_spent <- tracking_data |>
  group_by(domain) |>
  summarise(total_duration_hours = sum(duration) / 3600, .groups = 'drop') |>
  arrange(desc(total_duration_hours))

# Select the top 20 websites by total time spent in hours
top_websites_time_spent <- head(website_time_spent, 20)

# Create a histogram for time spent in hours
top_web_time_spent_plot <- ggplot(top_websites_time_spent, aes(x = reorder(domain, total_duration_hours), y = total_duration_hours)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top 20 Websites with the Most Total Time Spent (in hours)",
       x = "Website",
       y = "Total Time Spent (in hours)") +
  theme_minimal()

top_web_time_spent_plot

```

- need to remove ads & tracker websites
- if can add by wave --> can see change over time (do facet plot)

## Social media
```{r social media}

#  regex for  social media platforms
social_media_patterns <- c(
  "facebook" = "facebook\\.com",
  "twitter" = "twitter\\.com",
  "instagram" = "instagram\\.com",
  "linkedin" = "linkedin\\.com",
  "youtube" = "youtube\\.com",
  "snapchat" = "snapchat\\.com",
  "tiktok" = "tiktok\\.com"
)

# Filter tracking_data 
social_media_visits <- tracking_data |>
  filter(str_detect(domain, paste(social_media_patterns, collapse = "|"))) |>
  mutate(platform = str_extract(domain, paste(names(social_media_patterns), collapse = "|"))) |>
  group_by(platform) |>
  summarise(visit_count = n(), .groups = 'drop') |>
  arrange(desc(visit_count))

# histogram
social_media_plot <- ggplot(social_media_visits, aes(x = reorder(platform, visit_count), y = visit_count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Visits to Major Social Media Platforms",
       x = "Social Media Platform",
       y = "Number of Visits") +
  theme_minimal()
social_media_plot
```

# News sites

I tried fiddling with everything here, I don't understand why it appears as a single block....
```{r news sites}

# regex for major and minor U.S. news sites
news_sites_patterns <- c(
  "CNN" = "cnn\\.com",
  "Fox News" = "foxnews\\.com",
  "NBC News" = "nbcnews\\.com",
  "CBS News" = "cbsnews\\.com",
  "ABC News" = "abcnews\\.go\\.com",
  "USA Today" = "usatoday\\.com",
  "The New York Times" = "nytimes\\.com",
  "The Washington Post" = "washingtonpost\\.com",
  "The Wall Street Journal" = "wsj\\.com",
  "Los Angeles Times" = "latimes\\.com",
  "The Boston Globe" = "bostonglobe\\.com",
  "Chicago Tribune" = "chicagotribune\\.com",
  "The Atlantic" = "theatlantic\\.com",
  "The Guardian US" = "theguardian\\.com/us",
  "BuzzFeed News" = "buzzfeednews\\.com",
  "HuffPost" = "huffpost\\.com"
)

# rilter tracking_data for visits to news sites
news_site_visits <- tracking_data |>
  filter(str_detect(domain, paste(news_sites_patterns, collapse = "|"))) |>
  mutate(news_site = str_extract(domain, paste(names(news_sites_patterns), collapse = "|"))) |>
  group_by(news_site) |>
  summarise(visit_count = n(), .groups = 'drop') |>
  arrange(desc(visit_count))

# histogram
news_plot <- ggplot(news_site_visits, aes(x = reorder(news_site, visit_count), y = visit_count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Visits to Major and Minor U.S. News Sites",
       x = "News Site",
       y = "Number of Visits") +
  theme_minimal()
news_plot
```

```{r legacy vs new media}

# define domains for legacy VS new media
legacy_media_domains <- c("cnn.com", 
                          "nytimes.com", 
                          "washingtonpost.com", 
                          "wsj.com", 
                          "latimes.com", 
                          "bbc.co.uk", 
                          "nbcnews.com", 
                          "abcnews.go.com", 
                          "cbsnews.com", 
                          "foxnews.com")
new_media_domains <- c("buzzfeed.com", 
                       "huffpost.com", 
                       "vox.com", 
                       "vice.com", 
                       "axios.com", 
                       "thedailybeast.com", 
                       "breitbart.com", 
                       "theverge.com", 
                       "mashable.com", 
                       "gizmodo.com")

# classify visits 
tracking_data <- tracking_data |> 
  mutate(media_type = case_when(
    str_detect(domain, paste(legacy_media_domains, collapse = "|")) ~ "Legacy Media",
    str_detect(domain, paste(new_media_domains, collapse = "|")) ~ "New Media",
    TRUE ~ "Other"
  ))

# Count visits in each category
media_visits <- tracking_data |> 
  group_by(media_type) |> 
  summarise(visit_count = n(), .groups = 'drop')

# histogram with legacy vs new vs other
legacy_new_other_plot <- ggplot(media_visits, aes(x = media_type, y = visit_count, fill = media_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Visits to Legacy vs New Media Companies",
       x = "Media Type",
       y = "Number of Visits") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()
legacy_new_other_plot 

# filter out'other' category, so just news comparison
legacy_new_media_visits <- media_visits %>%
  filter(media_type %in% c("Legacy Media", "New Media"))

# histogram for legacy vs new media
legacy_new_plot <- ggplot(legacy_new_media_visits, aes(x = media_type, y = visit_count, fill = media_type)) +
  geom_bar(stat = "identity") +
  labs(title = "Visits to Legacy vs New Media Companies",
       x = "Media Type",
       y = "Number of Visits") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal()
legacy_new_plot 
```

## Other Categories

- Social Media
- Political Parties
- Advocacy Groups
- Government Sites
- Political News
- Political Blogs
- Fact CHecking sites
- Campaign sites
- Issue-specific sites
- international politics (remove?)
- social issies
- other

idea is to give overview of what's being used, but I've set this to eval = F because it's too computationally expensive (ie my comp just crashes - find more efficient way?)

```{r eval = F}

# regex patterns for a variety of different possible categories of interest
categories <- list(
  "Social Media" = "facebook\\.com |twitter\\.com|instagram\\.com|linkedin\\.com|youtube\\.com|snapchat\\.com|tiktok\\.com", 
  
  "Political Parties" = "democrats\\.org|gop\\.com",
  
  "Advocacy Groups" = "aclu\\.org|nra\\.org|moveon\\.org|heritage\\.org|cato\\.org|naacp\\.org|splcenter\\.org|eff\\.org|sierraclub\\.org|ppfa\\.org|amnesty\\.org|hrw\\.org|aclj\\.org|aflcio\\.org|commoncause\\.org|democracy21\\.org|epi\\.org|freespeechforpeople\\.org|publiccitizen\\.org|sunlightfoundation\\.com|taxfoundation\\.org|taxpolicycenter\\.org|brookings\\.edu|urban\\.org|aei\\.org|hoover\\.org|rand\\.org|cfpb\\.gov|epa\\.gov|fcc\\.gov|ftc\\.gov|fec\\.gov|gao\\.gov|govtrack\\.us|ic3\\.gov|ice\\.gov|irs\\.gov|justice\\.gov|lsc\\.gov|medicaid\\.gov|medicare\\.gov|nasa\\.gov|nih\\.gov|nlrb\\.gov|nsf\\.gov|ope\\.ed\\.gov|osha\\.gov|ssa\\.gov|supremecourt\\.gov|trade\\.gov|treasury\\.gov|usaid\\.gov|uscis\\.gov|usda\\.gov|usdoj\\.gov|usds\\.gov|usembassy\\.gov|usgs\\.gov|uspto\\.gov|va\\.gov|whitehouse\\.gov|aclu\\.org|eff\\.org|hrw\\.org|splcenter\\.org|adl\\.org|aflcio\\.org|amnesty\\.org|earthjustice\\.org|greenpeace\\.org|humanrightsfirst\\.org|nrdc\\.org|sierraclub\\.org|southerncenter\\.org|sunlightfoundation\\.com|unicef\\.org|unwomen\\.org",
  
  "Government Sites" = "\\.gov",
  
  "Political News" = "politico\\.com|realclearpolitics\\.com|thehill\\.com|dailykos\\.com|townhall\\.com|nationalreview\\.com|vox\\.com|msnbc\\.com|breitbart\\.com|mediamatters\\.org|newsmax\\.com|theintercept\\.com|motherjones\\.com|thinkprogress\\.org|reason\\.com|salon\\.com|slate\\.com|redstate\\.com|talkingpointsmemo\\.com|rawstory\\.com|alternet\\.org|conservativereview\\.com|theamericanconservative\\.com|drudgereport\\.com|freebeacon\\.com|theblaze\\.com|thefederalist\\.com|lawfareblog\\.com|justsecurity\\.org|judicialwatch\\.org|splcenter\\.org|aclj\\.org|heritage\\.org|cato\\.org|aei\\.org|brookings\\.edu|csis\\.org|rand\\.org|ssrc\\.org|cfr\\.org|carnegieendowment\\.org|wilsoncenter\\.org|urban\\.org|prri\\.org|pewresearch\\.org|pewforum\\.org|fivethirtyeight\\.com|axios\\.com|quartz\\.com|propublica\\.org|factcheck\\.org|opensecrets\\.org|sunlightfoundation\\.com|transparency\\.org|icij\\.org|occrp\\.org|bellingcat\\.com|commondreams\\.org|democracynow\\.org|truthout\\.org|truthdig\\.com|thenation\\.com|jacobinmag\\.com|newrepublic\\.com|nymag\\.com|vanityfair\\.com|washingtonmonthly\\.com|theatlantic\\.com|economist\\.com|ft\\.com|guardian\\.co\\.uk|independent\\.co\\.uk|telegraph\\.co\\.uk|times\\.co\\.uk|bbc\\.co\\.uk|aljazeera\\.com|rt\\.com|sputniknews\\.com|dw\\.com|france24\\.com|euronews\\.com|reuters\\.com|apnews\\.com|afp\\.com|bloomberg\\.com|cnbc\\.com|fortune\\.com|businessinsider\\.com|marketwatch\\.com|wsj\\.com|ft\\.com|nytimes\\.com|washingtonpost\\.com|latimes\\.com|chicagotribune\\.com|usatoday\\.com|newsweek\\.com|time\\.com|newyorker\\.com|forbes\\.com|nypost\\.com|thedailybeast\\.com|huffpost\\.com|buzzfeed\\.com",
  
  "Political Blogs" = "huffpost\\.com|breitbart\\.com|mediamatters\\.org|newsmax\\.com|theintercept\\.com|reason\\.com|salon\\.com|slate\\.com|redstate\\.com|talkingpointsmemo\\.com|rawstory\\.com|alternet\\.org|dailykos\\.com|thinkprogress\\.org|motherjones\\.com|truthdig\\.com|truthout\\.com|democracynow\\.org|commondreams\\.org|jacobinmag\\.com|thenation\\.com|progressive\\.org|counterpunch\\.org|thebaffler\\.com|dissentmagazine\\.org|inthesetimes\\.com|newrepublic\\.com|nymag\\.com|politico\\.com|realclearpolitics\\.com|thehill\\.com|vox\\.com|theatlantic\\.com|propublica\\.org|quartz\\.com|axios\\.com|buzzfeednews\\.com|fivethirtyeight\\.com|theconversation\\.com|thedailybeast\\.com|lawfareblog\\.com|justsecurity\\.org|thebulwark\\.com|thebrowser\\.com|stratechery\\.com",
  
  "Fact-Checking Sites" = "factcheck\\.org|snopes\\.com|politifact\\.com|fullfact\\.org|checkyourfact\\.com|hoax-slayer\\.net|truthorfiction\\.com|leadstories\\.com|mediabiasfactcheck\\.com|opensources\\.co|logicalfallacy\\.info|flackcheck\\.org|washingtonpost\\.com/news/fact-checker|nytimes\\.com/section/politics/fact-check|apnews\\.com/APFactCheck|reuters\\.com/fact-check|bbc\\.co\\.uk/news/reality_check|channel4\\.com/news/factcheck|theguardian\\.com/us-news/series/fact-check-usa|abcnews\\.go\\.com/Politics/fact-check-news|cnn\\.com/factsfirst|politico\\.eu/tag/fact-check|vox\\.com/fact-check|buzzfeed\\.com/tag/fact-check|msnbc\\.com/fact-check",
  
  "Campaign Sites" = "joebiden\\.com|donaldjtrump\\.com",
  
  "Issue-Specific Sites" = "sierraclub\\.org|nrdc\\.org|edf\\.org|greenpeace\\.org|350\\.org|earthjustice\\.org|ema-online\\.org|oceana\\.org|audubon\\.org|nationalgeographic\\.com|climatecentral\\.org|gofossilfree\\.org|nra\\.org|gunowners\\.org|bradyunited\\.org|everytown\\.org|giffords\\.org|thetrace\\.org|plannedparenthood\\.org|prochoiceamerica\\.org|guttmacher\\.org|aclj\\.org|allianceforjustice\\.org|lamda\\.org|glaad\\.org|hrc\\.org|thetaskforce\\.org|freedomtomarry\\.org|now\\.org|womensmarch\\.com|emilyslist\\.org|blm\\.org|naacp\\.org|colorofchange\\.org|splcenter\\.org|adl\\.org|maldef\\.org|lulac\\.org|aarp\\.org|ama-assn\\.org|healthcare\\.gov|kff\\.org|commonwealthfund\\.org|urban\\.org|cbpp\\.org|epi\\.org|aflcio\\.org|change\\.org|avaaz\\.org|amnesty\\.org|hrw\\.org|doctorswithoutborders\\.org|redcross\\.org|oxfam\\.org|care\\.org|savethechildren\\.org|unicef\\.org|humanrightsfirst\\.org",
  
  "International Politics" = "aljazeera\\.com|rt\\.com|sputniknews\\.com|dw\\.com|france24\\.com|euronews\\.com|reuters\\.com|apnews\\.com|afp\\.com|bbc\\.co\\.uk|theguardian\\.com|int\\.reuters\\.com|chinadaily\\.com\\.cn|xinhuanet\\.com|english\\.kyodonews\\.net|nikkei\\.com|asiatimes\\.com|scmp\\.com|hindustantimes\\.com|timesofindia\\.indiatimes\\.com|thehindu\\.com|indianexpress\\.com|dawn\\.com|al-monitor\\.com|haaretz\\.com|jpost\\.com|timesofisrael\\.com|middleeasteye\\.net|telesurenglish\\.net|english\\.alarabiya\\.net|turkishminute\\.com|dailynews\\.eg|thenationalnews\\.com|arabnews\\.com|africanews\\.com|allafrica\\.com|mg\\.co\\.za|standardmedia\\.co\\.ke|nation\\.co\\.ke|news24\\.com|theeastafrican\\.co\\.ke|iol\\.co\\.za|theafricareport\\.com|naijanews\\.com|pulse\\.ng|ghanaweb\\.com|graphic\\.com\\.gh|vanguardngr\\.com|punchng\\.com|premiumtimesng\\.com|thisdaylive\\.com|businessday\\.ng|thenewhumanitarian\\.org|allafrica\\.com|mailandguardian\\.co\\.za",
  
  "Social Issues" = "naacp\\.org|aclu\\.org|hrc\\.org|amnesty\\.org|hrw\\.org|splcenter\\.org|adl\\.org|maldef\\.org|now\\.org|nclr\\.org|glaad\\.org|lamdba\\.org|unwomen\\.org|girlsnotbrides\\.org|womensenews\\.org|msfoundation\\.org|ruthelliscenter\\.org|glad\\.org|transgenderlawcenter\\.org|transequality\\.org|thetrevorproject\\.org|itgetsbetter\\.org|glsen\\.org|pflag\\.org|stonewall\\.org\\.uk|mermaidsuk\\.org\\.uk|genderedintelligence\\.co\\.uk|blm\\.org|colorofchange\\.org|raceforward\\.org|naacpldf\\.org|civilrights\\.org|nationalactionnetwork\\.net|blacklivesmatter\\.com|eji\\.org|brennancenter\\.org|fairvote\\.org|democracyawakening\\.org|commoncause\\.org|publiccitizen\\.org|citizensforethics\\.org|sunlightfoundation\\.com|opensecrets\\.org|maplight\\.org|propublica\\.org|accountable\\.us|greenpeace\\.org|sierraclub\\.org|earthjustice\\.org|nrdc\\.org|edf\\.org|foe\\.org|350\\.org|climaterealityproject\\.org|wwf\\.org|conservation\\.org|oceana\\.org|rainforest-alliance\\.org"
)


# Create a new column 'category_type' based on domain patterns
tracking_data <- tracking_data |>
  mutate(category_type = case_when(
    str_detect(domain, categories[["Social Media"]]) ~ "Social Media",
    str_detect(domain, categories[["Political Parties"]]) ~ "Political Parties",
    str_detect(domain, categories[["Advocacy Groups"]]) ~ "Advocacy Groups",
    str_detect(domain, categories[["Government Sites"]]) ~ "Government Sites",
    str_detect(domain, categories[["Political News"]]) ~ "Political News",
    str_detect(domain, categories[["Political Blogs"]]) ~ "Political Blogs",
    str_detect(domain, categories[["Fact-Checking Sites"]]) ~ "Fact-Checking Sites",
    str_detect(domain, categories[["Campaign Sites"]]) ~ "Campaign Sites",
    str_detect(domain, categories[["Issue-Specific Sites"]]) ~ "Issue-Specific Sites",
    str_detect(domain, categories[["International Politics"]]) ~ "International Politics",
    str_detect(domain, categories[["Social Issues"]]) ~ "Social Issues",
    TRUE ~ "Other"
  ))

# count visits in each category
category_visits <- tracking_data |> 
  group_by(category_type) |> 
  summarise(visit_count_2 = n(), .groups = 'drop')

# bar plot
category_plot <- ggplot(category_visits, aes(x = category_type, y = visit_count_2, fill = category_type)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Visits to diff types of sites",
       x = "Category",
       y = "Number of Visits") +
  theme_minimal()
category_plot

```